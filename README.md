# WebAssembly Performance Benchmark: Rust vs TinyGo

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Platform](https://img.shields.io/badge/platform-Linux%20%28AWS%20EC2%29-lightgrey.svg)
![Rust](https://img.shields.io/badge/rust-1.90-orange.svg)
![TinyGo](https://img.shields.io/badge/tinygo-0.39.0-00ADD8.svg)
![Node.js](https://img.shields.io/badge/node-24.7.0-green.svg)
![Python](https://img.shields.io/badge/python-3.11%2B-blue.svg)

A comprehensive benchmarking framework to evaluate the efficiency of **Rust** and **TinyGo** when compiled to WebAssembly across various computational workloads.

## üéØ Overview

This project provides a rigorous performance comparison between:

| Language | Target | Runtime | Optimization |
|----------|--------|---------|--------------|
| **Rust 1.90** | `wasm32-unknown-unknown` | Zero-cost abstractions, no GC | `-O3`, fat LTO |
| **TinyGo 0.39** | `wasm` | Garbage collected runtime | `-opt=2`, no scheduler |

**Test Environment:**

- **Hardware:** AWS EC2 c7g.2xlarge (4 CPU, 16GB RAM)
- **OS:** Ubuntu 22.04 (Linux/x86_64)
- **Runtime:** Headless Chromium v140+ (Puppeteer)
- **Toolchain:** Rust 1.90, Go 1.25.1, TinyGo 0.39, Node.js 24.7, Python 3.11+

## üöÄ Quick Start

```bash
# 1. Install dependencies
make check deps  # Verify required tools
make init        # Install project dependencies

# 2. Build WebAssembly modules
make build       # Build all WASM modules for both languages

# 3. Run benchmarks
make run quick   # Fast development test (~5-10s)
make run         # Full benchmark suite (~30+ minutes)

# 4. Analyze results
make qc          # Quality control validation
make analyze     # Statistical analysis and visualization

# All-in-one execution
make all quick   # Complete pipeline with quick settings
make all         # Complete research-grade pipeline
```

## ‚öôÔ∏è Installation & Setup

### üíª **System Requirements**

- **Linux** Ubuntu 22.04+ (AWS EC2 c7g instances recommended)
- **Hardware**: 4 CPU cores, 16GB RAM minimum for benchmark execution

### üõ†Ô∏è **Required Components**

| Tool | Version | Purpose |
|------|---------|----------|
| **Rust** | 1.90+ | WASM compilation with `wasm32-unknown-unknown` target |
| **TinyGo** | 0.39+ | Go-to-WASM compilation |
| **Node.js** | 24+ | Test harness and automation |
| **Python** | 3.11+ | Statistical analysis with uv |
| **Binaryen** | Latest | `wasm-opt` optimization |
| **WABT** | Latest | `wasm-strip` binary processing |

## üèÉ Running Benchmarks

### üîÑ **Development Workflow**

```bash
# Quick validation (~5-10s)
make run quick

# With visible browser (debugging)
make run quick headed

# Full benchmark suite (~30+ minutes)
make run

# Complete experimental pipeline
make all     # build ‚Üí run ‚Üí qc ‚Üí analyze ‚Üí plots
```

### üìã **Command Reference**

| Command | Purpose | Duration | Use Case |
|---------|---------|----------|----------|
| `make run quick` | Fast development test | 5-10s | Development validation |
| `make run` | Full benchmark suite | 30+ min | Research analysis |
| `make run quick headed` | Debug with visible browser | 5-10s | Troubleshooting |
| `make all quick` | Complete pipeline (quick) | 5-8 min | CI/CD validation |
| `make all` | Complete research pipeline | 45-60 min | Publication data |

### ‚öôÔ∏è **Advanced Options**

```bash
# Custom benchmark execution
node scripts/run_bench.js --parallel --max-concurrent=4
node scripts/run_bench.js --verbose
node scripts/run_bench.js --timeout=120000

# Configuration editing
# Edit configs/bench.yaml or configs/bench-quick.yaml
```

## üê≥ Docker Setup (Recommended)

For the easiest setup experience, use the provided Docker containerization that provides a fully isolated, pre-configured development and benchmarking environment.

### üìã Prerequisites

- **Docker Desktop** installed and running (or Docker Engine on Linux)
- **Minimum Resources**: 4GB RAM allocated to Docker, 10GB free disk space
- **Recommended**: 8GB+ RAM for optimal benchmark performance

### üèóÔ∏è Environment Overview

The Docker container includes:

- **Pre-installed toolchains**: Rust 1.90, TinyGo 0.39, Go 1.25.1, Node.js 24.7, Python 3.11+
- **All dependencies**: uv, pnpm packages, system libraries
- **Isolated workspace**: Persistent data volumes for results and builds
- **Development tools**: Full development environment with debugging capabilities

### üöÄ Quick Docker Start

```bash

# step-by-step approach for development:
make docker start     # Build and start container with health checks
make docker init      # Initialize development environment
make docker build     # Build WebAssembly modules
make docker run       # Execute benchmarks
make docker analyze   # Run statistical analysis and generate reports

# One-command complete pipeline 
make docker full
```

### üõ†Ô∏è Development Workflow

```bash
# Enter container for interactive development
make docker shell

# Build with specific options
make docker build rust        # Build only Rust modules
make docker build tinygo      # Build only TinyGo modules
make docker build parallel    # Parallel build for faster compilation

# Run benchmarks with different configurations
make docker run quick         # Fast development testing (~5-10s)
make docker run               # Full benchmark suite (~30+ minutes)

# Quality control and analysis
make docker qc                # Quality control validation
make docker stats             # Statistical analysis
make docker plots             # Generate visualization charts

# Testing and validation
make docker test              # Run test suite
make docker validate          # Cross-language validation
```

### üê≥ Container Management

```bash
# Container lifecycle
make docker start              # Start container with health verification
make docker stop               # Gracefully stop container
make docker restart            # Restart container
make docker status             # Show container status and resource usage
make docker logs               # Display recent container logs

# Information and cleanup
make docker info               # Show system information from container
make docker clean              # Clean containers and images
make docker help               # Display help information
```

### üíæ Data Persistence

- **Results**: Benchmark data persists in `results/` directory
- **Builds**: Compiled WASM modules saved in `builds/` directory
- **Reports**: Analysis outputs available in `reports/` directory
- **Environment**: Toolchain versions locked in container

### ‚ö° Benefits

- **Zero configuration**: All dependencies pre-installed and configured
- **Consistent environment**: Same setup across different host systems
- **Isolated development**: No conflicts with host system packages
- **Performance optimization**: Containerized environment tuned for benchmarks
- **Data persistence**: Results and builds survive container restarts
- **Easy cleanup**: Complete environment isolation with simple cleanup

### üìñ Troubleshooting

```bash
# Check container status
make docker status

# View container logs for debugging
make docker logs

# Restart if issues occur
make docker restart

# Clean and rebuild if needed
make docker clean
make docker start
```

See [`docs/docker-setup.md`](docs/docker-setup.md) for detailed Docker documentation and advanced configuration options.

## üìä Benchmark Tasks

Three representative WebAssembly workloads designed to compare language performance:

### üî¢ **Mandelbrot Set Computation**

- **Type**: CPU-intensive floating-point operations
- **Purpose**: Test scalar math performance and loop optimization
- **Scales**:
  - **micro**: 64√ó64, 100 iterations
  - **small**: 256√ó256, 500 iterations
  - **medium**: 512√ó512, 1000 iterations
  - **large**: 1024√ó1024, 2000 iterations
- **Verification**: FNV-1a hash on iteration counts

### üìù **JSON Parsing & Processing**

- **Type**: String processing and memory allocation
- **Purpose**: Test parsing performance and garbage collection impact
- **Scales**:
  - **micro**: 500 records
  - **small**: 5,000 records
  - **medium**: 15,000 records
  - **large**: 30,000 records
- **Schema**: Fields: id, value, flag, name with sequential/random/derived patterns
- **Verification**: Hash of aggregated field values

### ‚ö° **Matrix Multiplication**

- **Type**: Dense numerical computation with memory access patterns
- **Purpose**: Test compute-intensive algorithms and cache utilization
- **Scales**:
  - **micro**: 64√ó64 matrices
  - **small**: 256√ó256 matrices
  - **medium**: 384√ó384 matrices
  - **large**: 576√ó576 matrices
- **Verification**: Hash of result matrix elements

**üéØ Design Principle**: Identical algorithms and verification across languages ensure fair comparison.

## üìÅ Project Structure

```text
wasm-benchmark/
‚îú‚îÄ‚îÄ üì¶ tasks/                    # Benchmark implementations
‚îÇ   ‚îú‚îÄ‚îÄ mandelbrot/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rust/                # Rust WASM implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/lib.rs       # Main benchmark entry point
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/mandelbrot.rs # Core algorithm
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/hash.rs      # FNV-1a hashing
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Cargo.toml       # Rust configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tinygo/              # TinyGo WASM implementation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.go          # Main benchmark entry point
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main_test.go     # Unit tests
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ go.mod           # Go module configuration
‚îÇ   ‚îú‚îÄ‚îÄ json_parse/              # JSON parsing benchmark
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rust/src/            # Rust parser, generator, types
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tinygo/              # TinyGo implementation
‚îÇ   ‚îî‚îÄ‚îÄ matrix_mul/              # Matrix multiplication
‚îÇ       ‚îú‚îÄ‚îÄ rust/src/            # Rust matrix operations
‚îÇ       ‚îî‚îÄ‚îÄ tinygo/              # TinyGo implementation
‚îú‚îÄ‚îÄ üîß scripts/                  # Build and automation
‚îÇ   ‚îú‚îÄ‚îÄ build_all.sh            # Complete build pipeline
‚îÇ   ‚îú‚îÄ‚îÄ build_rust.sh           # Rust-specific builds
‚îÇ   ‚îú‚îÄ‚îÄ build_tinygo.sh         # TinyGo-specific builds
‚îÇ   ‚îú‚îÄ‚îÄ build_config.js         # Configuration file generation
‚îÇ   ‚îú‚îÄ‚îÄ run_bench.js            # Main benchmark orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ fingerprint.sh          # Environment fingerprinting
‚îÇ   ‚îú‚îÄ‚îÄ validate_*.sh           # Task validation scripts
‚îÇ   ‚îú‚îÄ‚îÄ services/               # Service architecture (DI pattern)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BrowserService.js   # Browser automation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ResultsService.js   # Result collection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ LoggingService.js   # Structured logging
‚îÇ   ‚îî‚îÄ‚îÄ interfaces/             # Service interfaces
‚îú‚îÄ‚îÄ üß™ harness/web/             # Browser test environment
‚îÇ   ‚îú‚îÄ‚îÄ bench.html             # Test execution page
‚îÇ   ‚îú‚îÄ‚îÄ bench.js               # Browser-side test logic
‚îÇ   ‚îú‚îÄ‚îÄ config_loader.js       # Configuration management
‚îÇ   ‚îî‚îÄ‚îÄ wasm_loader.js         # WebAssembly module loading
‚îú‚îÄ‚îÄ üìä analysis/                # Statistical analysis pipeline
‚îÇ   ‚îú‚îÄ‚îÄ qc.py                  # Quality control & outlier detection
‚îÇ   ‚îú‚îÄ‚îÄ statistics.py          # Welch's t-test, Cohen's d analysis
‚îÇ   ‚îú‚îÄ‚îÄ plots.py               # Matplotlib visualization generation
‚îÇ   ‚îú‚îÄ‚îÄ decision.py            # Decision support analysis
‚îÇ   ‚îú‚îÄ‚îÄ data_models.py         # Data structure definitions
‚îÇ   ‚îú‚îÄ‚îÄ validation.py          # Cross-language validation
‚îÇ   ‚îî‚îÄ‚îÄ common.py              # Shared utilities
‚îú‚îÄ‚îÄ üß™ tests/                  # Comprehensive test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/                  # Unit tests (Vitest)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config-parser.test.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statistics.test.js
‚îÇ   ‚îú‚îÄ‚îÄ integration/           # Cross-language validation
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cross-language.test.js
‚îÇ   ‚îú‚îÄ‚îÄ utils/                 # Test utilities
‚îÇ   ‚îî‚îÄ‚îÄ setup.js              # Test configuration
‚îú‚îÄ‚îÄ ‚öôÔ∏è configs/                # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ bench.yaml            # Production benchmark config
‚îÇ   ‚îú‚îÄ‚îÄ bench.json            # Compiled JSON configuration
‚îÇ   ‚îú‚îÄ‚îÄ bench-quick.yaml      # Development/CI config
‚îÇ   ‚îî‚îÄ‚îÄ bench-quick.json      # Quick test configuration
‚îú‚îÄ‚îÄ üìà results/                # Benchmark output data
‚îú‚îÄ‚îÄ üìã reports/                # Generated reports and analysis
‚îÇ   ‚îú‚îÄ‚îÄ plots/                 # Visualization outputs
‚îÇ   ‚îî‚îÄ‚îÄ statistics/            # Statistical analysis reports
‚îú‚îÄ‚îÄ üóÇÔ∏è data/                  # Reference data and validation
‚îÇ   ‚îî‚îÄ‚îÄ reference_hashes/      # Cross-language validation hashes
‚îú‚îÄ‚îÄ üèóÔ∏è builds/                # Compiled WebAssembly modules
‚îÇ   ‚îú‚îÄ‚îÄ rust/                  # Optimized Rust WASM files
‚îÇ   ‚îî‚îÄ‚îÄ tinygo/                # Optimized TinyGo WASM files
‚îú‚îÄ‚îÄ meta.json                  # Experiment metadata
‚îú‚îÄ‚îÄ versions.lock              # Toolchain version lock
‚îú‚îÄ‚îÄ pyproject.toml            # Python dependencies (uv)
‚îú‚îÄ‚îÄ package.json              # Node.js dependencies
‚îú‚îÄ‚îÄ vitest.config.js          # Test framework configuration
‚îú‚îÄ‚îÄ eslint.config.js          # Code quality configuration
‚îî‚îÄ‚îÄ Makefile                  # Automated build and workflow
```

## üìö Documentation

This project includes comprehensive documentation in both English and Chinese to support different developer audiences:

### üìö **Core Documentation**

| Document | Language | Description | Link |
|----------|----------|-------------|------|
| **Command Reference** | English | Complete guide to all available commands, workflows, and troubleshooting | [`command-reference_en.md`](docs/command-reference_en.md) |
| **Statistical Terminology** | English | Comprehensive statistical concepts and methods used in the project | [`statistical-terminology_en.md`](docs/statistical-terminology_en.md) |
| **Statistical Design Implementation** | English | Detailed architecture and implementation of statistical analysis system | [`statistical-design-impl_en.md`](docs/statistical-design-impl_en.md) |

### üî¨ **Development & Research Documentation**

| Document | Language | Description | Link |
|----------|----------|-------------|------|
| **Development TODO** | English | Project development roadmap and implementation status | [`development-todo_en.md`](docs/development-todo_en.md) |
| **Experiment Plan** | English | Research methodology and experimental design | [`experiment-plan_en.md`](docs/experiment-plan_en.md) |
| **Quick Flow Guide** | English | Fast development and testing workflow | [`run-quick-flow_en.md`](docs/run-quick-flow_en.md) |

### ‚öôÔ∏è **Configuration Documentation**

| Document | Language | Description | Link |
|----------|----------|-------------|------|
| **Timeout Configuration** | English | Timeout settings and configuration guide | [`timeout-configuration_en.md`](docs/timeout-configuration_en.md) |

> **üí° Tip**: Start with the Command Reference for practical usage, then explore Statistical Terminology for methodology understanding.

---

## üîß Technical Implementation

### üîó **WebAssembly Interface**

Both languages export a unified **C-style interface** for fair comparison:

```c
void     init(uint32_t seed);           // Initialize PRNG
uint32_t alloc(uint32_t n_bytes);       // Allocate memory
uint32_t run_task(uint32_t params_ptr); // Execute & return result hash
```

### ‚ö° **Optimization Settings**

| Language | Target | Flags | Post-processing |
|----------|--------|-------|----------------|
| **Rust** | `wasm32-unknown-unknown` | `-O3`, fat LTO, 1 codegen unit | `wasm-strip`, `wasm-opt -O3` |
| **TinyGo** | `wasm` | `-opt=2`, panic trap, no debug | `wasm-strip`, `wasm-opt -Oz` |

### ‚úÖ **Result Verification**

To ensure **implementation correctness** and **cross-language consistency** in this performance comparison, the project employs a comprehensive validation framework. Since compiler optimizations and language-specific runtime behaviors can potentially affect computational results, establishing algorithmic equivalence is critical before comparing performance metrics.

**Cross-Language Validation Mechanism:**

The framework uses **FNV-1a Hash** algorithm to verify that both Rust and TinyGo implementations produce identical computational results across all benchmark tasks. This validation strategy includes **449 reference test vectors** systematically generated to cover diverse computational scenarios:

- **320 Mandelbrot vectors**: Covering various grid sizes (2√ó2 to 256√ó256), iteration counts (10-2000), complex plane regions, and zoom scales to validate floating-point computation consistency
- **112 JSON Parse vectors**: Testing different record counts (0-65,535), seed variations, and edge cases to ensure parsing logic and data structure handling equivalence  
- **17 Matrix Mul vectors**: Spanning matrix dimensions (1√ó1 to 128√ó128) with varied seeds to validate numerical computation and memory access patterns

**Purpose**: This comprehensive validation ensures that any observed performance differences stem purely from language/compiler efficiency rather than algorithmic discrepancies, providing a fair and scientifically rigorous foundation for the benchmark comparison.

## üìä Statistical Methodology

### üîç **Quality Control Pipeline**

- **Outlier Detection**: IQR-based filtering (Q1-1.5√óIQR, Q3+1.5√óIQR)
- **Stability Validation**: Coefficient of Variation < 15% threshold
- **Sample Size**: Minimum 30 valid measurements per condition
- **Cross-Language Verification**: Hash-based result consistency

### üìà **Statistical Analysis**

**Significance Testing**:

- **Welch's t-test** for unequal variances (more robust than Student's t-test)
- **p-value interpretation**: p < 0.05 for statistical significance
- **95% Confidence Intervals** for mean difference estimation

**Effect Size Analysis**:

- **Cohen's d** for practical significance assessment
- **Thresholds**: |d| < 0.2 (negligible), 0.2-0.5 (small), 0.5-0.8 (medium), ‚â•0.8 (large)
- **Decision Framework**: Statistical significance + effect size ‚Üí language recommendation

### ‚≠ê **Quality Standards**

| Metric | Threshold | Purpose |
|--------|-----------|----------|
| Coefficient of Variation | ‚â§ 15% | Measurement stability |
| Minimum Sample Size | ‚â• 30 | Statistical power |
| Success Rate | ‚â• 80% | Data reliability |
| Timeout Rate | ‚â§ 10% | System stability |

## üî¨ Reproducibility & Validation

### üñºÔ∏è **Environment Fingerprinting**

```bash
# Generate reproducible environment snapshot
./scripts/fingerprint.sh
# Outputs: meta.json, versions.lock
```

**Locked Versions:**

- All toolchain versions recorded (`versions.lock`)
- Dependency versions pinned (`pnpm-lock.yaml`, `uv.lock`)
- System information captured (`meta.json`)
- Random seeds fixed for deterministic data generation

### ‚úÖ **Validation Framework**

- ‚úÖ **Hash Verification**: FNV-1a algorithm ensures implementation correctness
- ‚úÖ **Cross-Language Consistency**: 449 reference test vectors
- ‚úÖ **Statistical Validation**: Automated quality control checks
- ‚úÖ **Audit Trail**: Complete logging of benchmark execution

**Two-Layer Validation Strategy:**

1. **Build Validation** (`make test validate` / `./scripts/validate-tasks.sh`)
   - Validates WASM build artifacts and reference hashes exist
   - Uses TinyGo compiler to verify algorithm consistency
   - Comprehensive test vectors (449 vectors across 3 tasks)
   - **Note**: `matrix_mul` shows partial compatibility (6/17 vectors pass)
     - Small matrices (‚â§4x4): Full consistency ‚úÖ
     - Larger matrices: Floating-point precision differences due to compiler optimization variations
     - This is a **known limitation** and does not affect benchmark validity

2. **Runtime Validation** (`make test` / browser integration tests)
   - Validates actual WASM execution in browser environment
   - Tests with benchmark-specific parameters (seed=12345)
   - All tasks including `matrix_mul` (256√ó256, 384√ó384, 576√ó576) achieve 100% hash consistency ‚úÖ
   - Validates memory management, performance characteristics, and error handling

**Why Both Are Necessary:**

- Build validation ensures implementation correctness across diverse inputs
- Runtime validation confirms production environment behavior
- Together they provide comprehensive quality assurance

## üéØ Project Status & Features

| Component | Status | Implementation |
|-----------|--------|----------------|
| **Statistical Analysis** | ‚úÖ Complete | Welch's t-test, Cohen's d, confidence intervals |
| **Quality Control** | ‚úÖ Complete | IQR outlier detection, CV validation |
| **Cross-Language Validation** | ‚úÖ Complete | 449 reference test vectors |
| **Visualization System** | ‚úÖ Complete | Bar charts, box plots, statistical tables |
| **Test Suite** | ‚úÖ Complete | Unit, integration |
| **Build System** | ‚úÖ Complete | Rust/TinyGo optimized builds |
| **Documentation** | ‚úÖ Complete | Comprehensive guides and references |

### ‚ùì **Getting Help**

- **Command Reference**: `make help`
- **System Check**: `make status` for environment validation
- **Dependency Verification**: `make check deps` for toolchain validation
- **Issue Reporting**: Include output from `make info` and `make status`

## üìà Results & Analysis

### **Visualization Outputs**

Generated in `reports/plots/`:

- **`execution_time_comparison.png`**: Bar charts with means, medians, error bars, and significance markers
- **`memory_usage_comparison.png`**: Memory consumption patterns with GC impact analysis
- **`effect_size_heatmap.png`**: Cohen's d effect size visualization with color-coded significance levels
- **`distribution_variance_analysis.png`**: Side-by-side box plots showing performance consistency and variance patterns
- `decision_summary.html`: Interactive HTML dashboard with comprehensive analysis results - [View Online](https://alleninnz.github.io/wasm-benchmark-site/)

### üîÑ **Analysis Pipeline**

The analysis system provides comprehensive statistical evaluation:

```bash
# Step-by-step analysis
make qc          # Quality control: IQR outlier detection, CV validation
make stats       # Statistical analysis: Welch's t-tests, Cohen's d effect sizes
make plots       # Visualization generation: 4 chart types + HTML dashboard
make analyze     # Complete pipeline: qc ‚Üí stats ‚Üí plots ‚Üí decision support

# Direct analysis execution
python analysis/qc.py          # Outlier detection and quality validation
python analysis/statistics.py  # Statistical significance testing
python analysis/plots.py       # Matplotlib chart generation
```

**‚ú® Analysis Features:**

- **Quality Control**: Coefficient of variation analysis, outlier detection, success rate validation
- **Statistical Testing**: Welch's t-tests for unequal variances, 95% confidence intervals
- **Effect Size Analysis**: Cohen's d calculation with practical significance thresholds
- **Visualization Suite**: 4 chart types supporting engineering decision-making
- **Decision Support**: Automated language recommendation based on statistical evidence

## ‚ö†Ô∏è Limitations & Considerations

### üåç **Environmental Factors**

- **Single-platform Testing**: Results from AWS EC2 Linux/Chromium environment
- **System Interference**: Background processes may affect timing precision
- **Browser Variations**: Results specific to Chromium V8 WebAssembly implementation

### üéØ **Benchmark Scope**

- **Limited Task Coverage**: Three computational patterns (not comprehensive)
- **No I/O Testing**: Focus on CPU/memory intensive workloads only
- **Single-threaded**: No multi-threading evaluation
- **Memory Model**: JavaScript-WASM boundary overhead not isolated
- **Variance Heterogeneity**: Welch's t-test accounts for unequal variances between languages
- **GC Impact**: TinyGo's garbage collector introduces measurement variability (higher CV thresholds)

### üî¨ **Known Implementation Differences**

- **Matrix Multiplication (`matrix_mul`)**:
  - Rust and TinyGo implementations show **compiler-specific floating-point behavior**
  - Small matrices (‚â§4√ó4): Perfect consistency across all test vectors
  - Medium-to-large matrices: Precision differences emerge with certain random seeds
  - **Benchmark validation (256√ó256+ with seed=12345)**: 100% consistent ‚úÖ
  - **Comprehensive test vectors (17 cases, varied seeds/sizes)**: 35% match rate
  - This reflects real-world compiler optimization differences, not implementation errors
  - Benchmark results remain valid for production use cases with fixed parameters

## üìÑ License

This project is licensed under the **MIT License** - see the [`LICENSE`](LICENSE) file for details.

---

**Keywords:** WebAssembly, WASM, Rust, Go, TinyGo, Performance, Benchmark, Comparison, Statistical Analysis
